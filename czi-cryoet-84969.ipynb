{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "159f5376",
   "metadata": {
    "_cell_guid": "4cf02a6f-b7e9-4360-892d-b1a50793eb12",
    "_uuid": "2a1239f3-55fc-4dbb-9d3a-e90bfffa038c",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.915141,
     "end_time": "2024-11-18T18:31:18.805275",
     "exception": false,
     "start_time": "2024-11-18T18:31:16.890134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Derived from:\n",
    "https://github.com/cellcanvas/album-catalog/blob/main/solutions/copick/compare-picks/solution.py\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def compute_metrics(reference_points, reference_radius, candidate_points):\n",
    "    num_reference_particles = len(reference_points)\n",
    "    num_candidate_particles = len(candidate_points)\n",
    "\n",
    "    if len(reference_points) == 0:\n",
    "        return 0, num_candidate_particles, 0\n",
    "\n",
    "    if len(candidate_points) == 0:\n",
    "        return 0, 0, num_reference_particles\n",
    "\n",
    "    ref_tree = KDTree(reference_points)\n",
    "    candidate_tree = KDTree(candidate_points)\n",
    "    raw_matches = candidate_tree.query_ball_tree(ref_tree, r=reference_radius)\n",
    "    matches_within_threshold = []\n",
    "    for match in raw_matches:\n",
    "        matches_within_threshold.extend(match)\n",
    "    # Prevent submitting multiple matches per particle.\n",
    "    # This won't be be strictly correct in the (extremely rare) case where true particles\n",
    "    # are very close to each other.\n",
    "    matches_within_threshold = set(matches_within_threshold)\n",
    "    tp = int(len(matches_within_threshold))\n",
    "    fp = int(num_candidate_particles - tp)\n",
    "    fn = int(num_reference_particles - tp)\n",
    "    return tp, fp, fn\n",
    "\n",
    "\n",
    "def score(\n",
    "        solution: pd.DataFrame,\n",
    "        submission: pd.DataFrame,\n",
    "        row_id_column_name: str,\n",
    "        distance_multiplier: float,\n",
    "        beta: int) -> float:\n",
    "    '''\n",
    "    F_beta\n",
    "      - a true positive occurs when\n",
    "         - (a) the predicted location is within a threshold of the particle radius, and\n",
    "         - (b) the correct `particle_type` is specified\n",
    "      - raw results (TP, FP, FN) are aggregated across all experiments for each particle type\n",
    "      - f_beta is calculated for each particle type\n",
    "      - individual f_beta scores are weighted by particle type for final score\n",
    "    '''\n",
    "\n",
    "    particle_radius = {\n",
    "        'apo-ferritin': 60,\n",
    "        'beta-amylase': 65,\n",
    "        'beta-galactosidase': 90,\n",
    "        'ribosome': 150,\n",
    "        'thyroglobulin': 130,\n",
    "        'virus-like-particle': 135,\n",
    "    }\n",
    "\n",
    "    weights = {\n",
    "        'apo-ferritin': 1,\n",
    "        'beta-amylase': 0,\n",
    "        'beta-galactosidase': 2,\n",
    "        'ribosome': 1,\n",
    "        'thyroglobulin': 2,\n",
    "        'virus-like-particle': 1,\n",
    "    }\n",
    "\n",
    "    particle_radius = {k: v * distance_multiplier for k, v in particle_radius.items()}\n",
    "\n",
    "    # Filter submission to only contain experiments found in the solution split\n",
    "    split_experiments = set(solution['experiment'].unique())\n",
    "    submission = submission.loc[submission['experiment'].isin(split_experiments)]\n",
    "\n",
    "    # Only allow known particle types\n",
    "    if not set(submission['particle_type'].unique()).issubset(set(weights.keys())):\n",
    "        raise ParticipantVisibleError('Unrecognized `particle_type`.')\n",
    "\n",
    "    assert solution.duplicated(subset=['experiment', 'x', 'y', 'z']).sum() == 0\n",
    "    assert particle_radius.keys() == weights.keys()\n",
    "\n",
    "    results = {}\n",
    "    for particle_type in solution['particle_type'].unique():\n",
    "        results[particle_type] = {\n",
    "            'total_tp': 0,\n",
    "            'total_fp': 0,\n",
    "            'total_fn': 0,\n",
    "        }\n",
    "\n",
    "    for experiment in split_experiments:\n",
    "        for particle_type in solution['particle_type'].unique():\n",
    "            reference_radius = particle_radius[particle_type]\n",
    "            select = (solution['experiment'] == experiment) & (solution['particle_type'] == particle_type)\n",
    "            reference_points = solution.loc[select, ['x', 'y', 'z']].values\n",
    "\n",
    "            select = (submission['experiment'] == experiment) & (submission['particle_type'] == particle_type)\n",
    "            candidate_points = submission.loc[select, ['x', 'y', 'z']].values\n",
    "\n",
    "            if len(reference_points) == 0:\n",
    "                reference_points = np.array([])\n",
    "                reference_radius = 1\n",
    "\n",
    "            if len(candidate_points) == 0:\n",
    "                candidate_points = np.array([])\n",
    "\n",
    "            tp, fp, fn = compute_metrics(reference_points, reference_radius, candidate_points)\n",
    "\n",
    "            results[particle_type]['total_tp'] += tp\n",
    "            results[particle_type]['total_fp'] += fp\n",
    "            results[particle_type]['total_fn'] += fn\n",
    "\n",
    "    aggregate_fbeta = 0.0\n",
    "    for particle_type, totals in results.items():\n",
    "        tp = totals['total_tp']\n",
    "        fp = totals['total_fp']\n",
    "        fn = totals['total_fn']\n",
    "\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        fbeta = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        aggregate_fbeta += fbeta * weights.get(particle_type, 1.0)\n",
    "\n",
    "    if weights:\n",
    "        aggregate_fbeta = aggregate_fbeta / sum(weights.values())\n",
    "    else:\n",
    "        aggregate_fbeta = aggregate_fbeta / len(results)\n",
    "    return aggregate_fbeta\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c96702-6e8b-4f0b-b937-331e38a6de4e",
   "metadata": {},
   "source": [
    "# Steps to Train ViTs Offline\n",
    "Download Model and Data:\n",
    "\n",
    "Downloaded the pre-trained ViT model and any required datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fd030cb-d1fe-4d52-85d6-5b232fdc2b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load and save the model locally\n",
    "model_name = \"google/vit-base-patch16-224\"\n",
    "image_processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "model = ViTForImageClassification.from_pretrained(model_name)\n",
    "\n",
    "# Save the model and processor locally\n",
    "image_processor.save_pretrained(\"./vit_model\")\n",
    "model.save_pretrained(\"./vit_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b351a3c-8df6-4e80-828f-61c88dc37cee",
   "metadata": {},
   "source": [
    "# Load Model Offline:\n",
    "\n",
    "After downloading, load the model and image processor from the local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18c39310-910d-40f0-a559-2a63a59f4833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "\n",
    "# Load the model and processor from the local directory\n",
    "image_processor = ViTImageProcessor.from_pretrained(\"./vit_model\")\n",
    "model = ViTForImageClassification.from_pretrained(\"./vit_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75046755-d162-4acb-93a9-c9b4ae8b1e9a",
   "metadata": {},
   "source": [
    "# Organize Training Data:\n",
    "\n",
    "Ensure the training images are organized in a directory structure like this:\n",
    "\n",
    "training_data/\n",
    "    apo-ferritin/\n",
    "        image1.png\n",
    "        image2.png\n",
    "        ...\n",
    "    beta-amylase/\n",
    "        image1.png\n",
    "        image2.png\n",
    "        ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa4323c-9e8e-4288-9b2c-50240035bb94",
   "metadata": {},
   "source": [
    "# Prepare Dataset:\n",
    "\n",
    "Create a custom dataset class to load images and labels based on the directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53d1a434-5555-4371-8a02-8130fab16d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CryoETDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.label_map = {\n",
    "            \"apo-ferritin\": 0,\n",
    "            \"beta-amylase\": 1,\n",
    "            \"beta-galactosidase\": 2,\n",
    "            \"ribosome\": 3,\n",
    "            \"thyroglobulin\": 4,\n",
    "            \"virus-like-particle\": 5\n",
    "        }\n",
    "\n",
    "        for label, idx in self.label_map.items():\n",
    "            label_dir = os.path.join(root_dir, label)\n",
    "            for img_name in os.listdir(label_dir):\n",
    "                self.image_paths.append(os.path.join(label_dir, img_name))\n",
    "                self.labels.append(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7fc40f-5757-4dfa-9425-55bc8bf540ff",
   "metadata": {},
   "source": [
    "# Transform Images:\n",
    "\n",
    "Apply transformations to prepare the images for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e84ef6e-d0e5-41ec-a18f-f2729b74c459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5230843-2e87-480e-89fc-41e7f445b960",
   "metadata": {},
   "source": [
    "# Load Dataset:\n",
    "\n",
    "Use the custom dataset class to load the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28973fbe-5228-4d7c-a11b-16c3c17f6e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = CryoETDataset(root_dir=\"training_data\", transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cda02ac-d820-4925-8e47-868205440db0",
   "metadata": {},
   "source": [
    "# Load Dataset:\n",
    "\n",
    "Use the custom dataset class to load the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7ea302-f670-42fa-9822-63f1b5f34400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = CryoETDataset(root_dir=\"training_data\", transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432790cc-2a9a-4984-9dff-4f9993c7c2b1",
   "metadata": {},
   "source": [
    "# Train the Model:\n",
    "\n",
    "Set up the training loop and train the Vision Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b50c26a-9489-450d-802a-7869d23a9b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import ViTForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "model_name = \"google/vit-base-patch16-224\"\n",
    "model = ViTForImageClassification.from_pretrained(model_name, num_labels=6)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562021ad-c2be-4760-af53-b121c322272a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.712396,
   "end_time": "2024-11-18T18:31:19.328931",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-18T18:31:13.616535",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
